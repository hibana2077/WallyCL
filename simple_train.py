#!/usr/bin/env python3
"""
ç°¡åŒ–çš„WallyCLè¨“ç·´è…³æœ¬ - åªç”¨CE lossï¼ŒæŒ‰ç…§è¨ºæ–·çµæœå¯¦ç¾
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import os
from pathlib import Path
import yaml
from tqdm import tqdm
import random
import wandb

# Import our modules
from src.dataset.ufgvc import UFGVCDataset
from src.models.wallycl import WallyClModel
from src.data.transforms import TRANSFORM_CONFIGS
from src.utils.metrics import WallyClMetrics


def load_config():
    """Load default config"""
    config_path = Path("configs/default.yaml")
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def simple_train():
    """ç°¡åŒ–çš„è¨“ç·´å‡½æ•¸ - åªç”¨CE loss"""
    print("="*60)
    print("WallyCL ç°¡åŒ–è¨“ç·´ - åªç”¨CE loss (ä¿®å¾©ç‰ˆ)")
    print("="*60)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    config = load_config()
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # åˆå§‹åŒ–wandb
    os.environ['WANDB_MODE'] = 'offline'
    wandb.init(
        project="wallycl-fixed",
        config=config,
        mode="offline"
    )
    
    # å‰µå»ºæ•¸æ“šé›†
    simple_transform = TRANSFORM_CONFIGS['val'](config['data']['input_size'])
    
    train_dataset = UFGVCDataset(
        dataset_name=config['data']['dataset_name'],
        root=config['data']['data_root'],
        split='train',
        transform=simple_transform,
        download=False
    )
    
    val_dataset = UFGVCDataset(
        dataset_name=config['data']['dataset_name'],
        root=config['data']['data_root'],
        split='val',
        transform=simple_transform,
        download=False
    )
    
    print(f"æ•¸æ“šé›†: Train={len(train_dataset)}, Val={len(val_dataset)}, Classes={len(train_dataset.classes)}")
    
    # æ›´æ–°é…ç½®
    config['data']['num_classes'] = len(train_dataset.classes)
    
    # å‰µå»ºæ¨¡å‹
    model = WallyClModel(
        model_name=config['model']['backbone'],
        num_classes=config['data']['num_classes'],
        embed_dim=config['model']['embed_dim'],
        hidden_dim=config['model']['hidden_dim'],
        k_tokens=config['model']['k_tokens'],
        tau_gumbel=config['model']['tau_gumbel'],
        pretrained=config['model']['pretrained']
    ).to(device)
    
    print(f"æ¨¡å‹åƒæ•¸: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    # è¨­ç½®å„ªåŒ–å™¨ - åˆ†å±¤å­¸ç¿’ç‡ï¼Œé‡é»è¨“ç·´åˆ†é¡é ­
    backbone_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if 'classifier' in name:
            classifier_params.append(param)
            print(f"åˆ†é¡é ­åƒæ•¸: {name}, shape: {param.shape}")
        else:
            backbone_params.append(param)
    
    # é«˜å­¸ç¿’ç‡åˆ†é¡é ­ï¼Œä½å­¸ç¿’ç‡backbone
    optimizer = optim.AdamW([
        {'params': backbone_params, 'lr': config['optimizer']['lr'] * 0.01},  # backbone: 5e-6
        {'params': classifier_params, 'lr': config['optimizer']['lr'] * 20}   # classifier: 1e-2
    ], weight_decay=config['optimizer']['weight_decay'])
    
    print(f"å„ªåŒ–å™¨è¨­ç½®:")
    print(f"  Backboneå­¸ç¿’ç‡: {config['optimizer']['lr'] * 0.01:.2e} ({len(backbone_params)}åƒæ•¸)")
    print(f"  åˆ†é¡é ­å­¸ç¿’ç‡: {config['optimizer']['lr'] * 20:.2e} ({len(classifier_params)}åƒæ•¸)")
    
    # åªç”¨CE loss
    criterion = nn.CrossEntropyLoss()
    
    # å‰µå»ºDataLoader
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)
    
    print(f"DataLoader: Train batches={len(train_loader)}, Val batches={len(val_loader)}")
    
    # è¨“ç·´
    print(f"\\né–‹å§‹è¨“ç·´ {config['training']['epochs']} epochs...")
    
    best_val_acc = 0.0
    
    for epoch in range(config['training']['epochs']):
        # è¨“ç·´éšæ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['training']['epochs']} Train")
        
        for i, (images, labels) in enumerate(pbar):
            images = images.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            
            # ç°¡åŒ–çš„å‰å‘å‚³æ’­ - ä¸ä½¿ç”¨tokenæ©Ÿåˆ¶
            outputs = model(images, return_tokens=False)
            logits = outputs['logits']
            
            loss = criterion(logits, labels)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # çµ±è¨ˆ
            train_loss += loss.item()
            _, preds = torch.max(logits, 1)
            train_correct += (preds == labels).sum().item()
            train_total += labels.size(0)
            
            # æ›´æ–°é€²åº¦æ¢
            train_acc = train_correct / train_total
            pbar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'acc': f"{train_acc:.4f}",
                'grad': f"{grad_norm:.2e}"
            })
            
            # è¨˜éŒ„ç¬¬ä¸€å€‹batchçš„è©³ç´°ä¿¡æ¯
            if i == 0:
                print(f"\\n  ç¬¬ä¸€å€‹batchè©³æƒ…:")
                print(f"    Loss: {loss.item():.4f}")
                print(f"    Logitsç¯„åœ: [{logits.min().item():.2f}, {logits.max().item():.2f}]")
                print(f"    æ¢¯åº¦ç¯„æ•¸: {grad_norm:.2e}")
                print(f"    é æ¸¬æ¨£æœ¬: {preds[:5].cpu().numpy()}")
                print(f"    çœŸå¯¦æ¨£æœ¬: {labels[:5].cpu().numpy()}")
        
        avg_train_loss = train_loss / len(train_loader)
        train_acc = train_correct / train_total
        
        # é©—è­‰éšæ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{config['training']['epochs']} Val")
            
            for images, labels in pbar:
                images = images.to(device)
                labels = labels.to(device)
                
                outputs = model(images, return_tokens=False)
                logits = outputs['logits']
                
                loss = criterion(logits, labels)
                
                val_loss += loss.item()
                _, preds = torch.max(logits, 1)
                val_correct += (preds == labels).sum().item()
                val_total += labels.size(0)
                
                # æ›´æ–°é€²åº¦æ¢
                val_acc = val_correct / val_total
                pbar.set_postfix({
                    'loss': f"{loss.item():.4f}",
                    'acc': f"{val_acc:.4f}"
                })
        
        avg_val_loss = val_loss / len(val_loader)
        val_acc = val_correct / val_total
        
        # è¨˜éŒ„çµæœ
        random_acc = 1.0 / config['data']['num_classes']
        
        print(f"\\nEpoch {epoch+1} çµæœ:")
        print(f"  Train: Loss={avg_train_loss:.4f}, Acc={train_acc:.4f}")
        print(f"  Val:   Loss={avg_val_loss:.4f}, Acc={val_acc:.4f}")
        print(f"  éš¨æ©Ÿæº–ç¢ºç‡: {random_acc:.4f}")
        print(f"  æå‡å€æ•¸: {val_acc/random_acc:.1f}x")
        
        # æ›´æ–°æœ€ä½³çµæœ
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            print(f"  âœ… æ–°çš„æœ€ä½³é©—è­‰æº–ç¢ºç‡ï¼")
        
        # wandbè¨˜éŒ„
        wandb.log({
            'epoch': epoch,
            'train_loss': avg_train_loss,
            'train_acc': train_acc,
            'val_loss': avg_val_loss,
            'val_acc': val_acc,
            'val_acc_vs_random': val_acc / random_acc
        })
        
        # æª¢æŸ¥æ˜¯å¦é¡¯è‘—å¥½æ–¼éš¨æ©Ÿ
        if val_acc > random_acc * 3:
            print(f"  ğŸ‰ æˆåŠŸï¼é©—è­‰æº–ç¢ºç‡é¡¯è‘—é«˜æ–¼éš¨æ©Ÿæ°´å¹³")
    
    print(f"\\nè¨“ç·´å®Œæˆï¼")
    print(f"æœ€ä½³é©—è­‰æº–ç¢ºç‡: {best_val_acc:.4f}")
    print(f"vs éš¨æ©Ÿæ°´å¹³: {random_acc:.4f} (æå‡ {best_val_acc/random_acc:.1f}x)")
    
    if best_val_acc > random_acc * 2:
        print("\\nâœ… ä¿®å¾©æˆåŠŸï¼ç¾åœ¨å¯ä»¥æ·»åŠ å…¶ä»–lossçµ„ä»¶äº†")
        return True
    else:
        print("\\nâŒ ä»éœ€è¦é€²ä¸€æ­¥èª¿è©¦")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    print("WallyCL ç°¡åŒ–è¨“ç·´è…³æœ¬")
    
    success = simple_train()
    
    if success:
        print("\\nğŸ“‹ ä¸‹ä¸€æ­¥è¨ˆåŠƒ:")
        print("1. é€æ­¥å¢åŠ å…¶ä»–lossæ¬Šé‡")
        print("2. ä¿®å¾©odd-one-outè©•ä¼°æ–¹å‘")
        print("3. æ¢å¾©å®Œæ•´çš„è¨“ç·´æµç¨‹")
    else:
        print("\\néœ€è¦é€²ä¸€æ­¥è¨ºæ–·æ¨¡å‹æˆ–æ•¸æ“šå•é¡Œ")


if __name__ == "__main__":
    main()
